% ---- introduction ----
\subsection{Überleitung auf explainability}
\todo{Write something about how humans need reasons to make a decision (see LIME sec 2)}
Building on the foundational understanding of anomalies, it becomes essential to explore how anomalies can be explained within systems, a process termed as explainable anomaly detection (XAD) \cite{Li2023}. This concept merges the two primary tasks:
\paragraph{\textit{Anomaly detection} can involve a detection model \todo{introduce models} or be based solely on expert analysis.}
\paragraph{\textit{Anomaly explanation} operates on these two scenarios: In the first scenario, methods like SHAP \cite{Lundberg2017} and LIME \cite{Ribeiro2016} help explain decisions made by complex models, aligning Detection-Definitions with Explanation-Definitions. In the second scenario, explanations focus on expert-identified anomalies without algorithmic input, striving to clarify why instances are considered anomalous. In summary, the primary distinction between these two cases lies in the focus of the explanation: whether it is on the model (and potentially the data) or exclusively on the data itself.}

As outlined by Doshi-Velez \& Kim \cite{DoshiVelez2017} and further detailed by Arrieta et al. \cite{BarredoArrieta2020} and Murdoch et al. \cite{Murdoch2019}, XAD ensures the transparency of AI systems, explaining decisions in human-understandable terms. 





\todo{add: explainability for generalization/ feature engineering (see LIME)}
As the reliance on anomaly detection systems continues to grow across various critical sectors, the significance of XAD becomes increasingly evident. In domains ranging from healthcare and financial services to automotive safety and aerospace, the deployment of these systems affects vital outcomes and decisions, where the stakes are exceptionally high. The ability to not only detect anomalies but also to understand and validate the underlying reasons behind such detections is imperative for fostering user trust and ensuring system accountability. By integrating XAD, we bridge the gap between sophisticated algorithmic performance and user comprehension, enabling stakeholders to undertake informed actions based on reliable and transparent anomaly assessments. Thus, the adoption of XAD isn't just a technological improvement—it is a fundamental requirement to enhance the efficacy, trustworthiness, and user acceptance of anomaly detection systems in our increasingly automated world.



% ---- Literature Review ----
\section{OPTIONAL: Explainable Anomaly Detection}
\subsection{What to Consider When Choosing an XAD Technique}
Selecting an appropriate XAD (Explainable Anomaly Detection) technique requires careful evaluation of several key factors. Once explanations are generated by an XAD method, their quality and trustworthiness must be assessed. Several studies in the realm of XAI (Explainable Artificial Intelligence) provide valuable insights in this regard. For instance, \cite{ref20, ref76} analyze the XAI literature and propose essential properties to consider when designing an XAI technique. Barbado et al. \cite{ref18} outline criteria for evaluating rule-extraction-based explanation methods, while Zhou et al. \cite{ref229} conduct a survey on the quality assessment of machine learning explanations. More recently, Sipple \& Youssef \cite{ref197} propose four desiderata for anomaly explanation methods and offer a framework for comparing different explanations.

However, there is no universal consensus on what constitutes an ideal XAD technique. Drawing from related work in XAI \cite{Li2023}, the following properties are particularly relevant when selecting or designing an XAD technique:

\begin{itemize}
    \item \textbf{Accuracy:} The ability to accurately classify unseen anomalous instances as anomalies.
    \item \textbf{Fidelity:} Consistency among the Oracle-Definition, Detection-Definition, and Explanation-Definition.
    \item \textbf{Comprehensibility:} The degree to which explanations are understandable to end-users.
    \item \textbf{Generality:} Compatibility with varying data types, data sizes, anomaly detection model types, model sizes, training regimes, or training restrictions.
    \item \textbf{Scalability:} The technique's ability to handle large input data sizes or extensive models.
    \item \textbf{Complexity:} The number of hyperparameters that end-users must configure.
\end{itemize}

The practical implementation and evaluation of XAD techniques largely depend on the application domain and end-users, which is beyond the scope of this survey. Nonetheless, considering these properties will help guide the selection of an appropriate XAD method.

% No universal best option (from survey)
In the domain of anomaly detection, the selection of appropriate techniques is highly context-dependent. This perspective is extensively supported by a comprehensive survey conducted by [Author(s)] (2023), which categorizes state-of-the-art techniques in explainable anomaly detection across various stages of the detection pipeline. The survey highlights the absence of a one-size-fits-all solution and instead emphasizes the importance of matching detection techniques to specific application needs \cite{Li2023}. The authors present a refined taxonomy that aids practitioners and researchers in identifying suitable methods based on a set of evaluation criteria including accuracy, fidelity, comprehensibility, generality, scalability, and complexity. This approach underscores the significance of considering the application domain and data specifics when selecting anomaly detection methods, acknowledging the vast diversity in potential applications and data types which profoundly influence the effectiveness of different techniques.

\subsection{Taxonomy from survey}
Anomaly detection remains crucial in identifying unexpected patterns across various domains, and the demand for explainability has led to the emergence of Explainable Anomaly Detection (XAD). The work by Li et al. \cite{10} provides a comprehensive taxonomy of XAD techniques across six dimensions, classifying them into \textit{Pre-model}, \textit{In-Model}, and \textit{Post-Model} approaches, with each dimension offering unique strategies for enhancing model interpretability.

\paragraph{Pre-model Techniques}
These techniques improve interpretability by refining feature selection and representation:
\begin{itemize}
    \item \textbf{Feature Selection:} CBRW\_FS and CBRW \cite{156,158}, IBFS \cite{223}, and the optimization framework by He \& Carbonell \cite{82} identify relevant feature subsets before anomaly detection. Correlation-based methods \cite{154,167} also explore feature dependencies.
    \item \textbf{Feature Representation:} Methods by Wu et al. \cite{219} and Dissanayake et al. \cite{60} aim to simplify models via high-level, interpretable representations, although deep models like CNNs sometimes hinder interpretability.
\end{itemize}

\paragraph{In-Model Techniques}
These techniques leverage inherently interpretable models for anomaly detection:
\begin{itemize}
    \item \textbf{Transparent Models:} Rule-based models \cite{83,230} and decision trees \cite{108,4,51} provide global explanations due to their transparent nature. Gaussian Processes \cite{21} and Generative Additive Models \cite{38} offer probabilistic and smooth function-based interpretations.
    \item \textbf{Feature Subset-Based Models:} Subspace anomaly detection techniques, such as OUTRES \cite{149} and HICS \cite{99}, identify anomalies in specific subspaces. CINFO \cite{157} and AE + SHAP \cite{178} combine feature selection with anomaly detection.
\end{itemize}

\paragraph{Post-Model Techniques}
These techniques interpret decisions after the anomaly detection process:
\begin{itemize}
    \item \textbf{Shallow Post-Model Techniques:} Subspace-based methods like LODA \cite{169} and surrogate models like LIME \cite{174} offer model-agnostic explanations, while SHAP \cite{128} highlights feature importance.
    \item \textbf{Deep Post-Model Techniques:} Denoising AutoEncoder with SHAP \cite{219} and graph learning \cite{116} are used to interpret deep learning models.
\end{itemize}

Li et al. \cite{10} emphasize the need for comprehensive, structured categorization in anomaly explanation, proposing a refined taxonomy to cover the diversity of XAD techniques. Building on this framework, our research focuses on \todo{include [selected model]}, which aligns with the \todo{include [Pre-model/In-Model/Post-Model]} category, providing both anomaly detection and interpretability.

\subsection{Explainable AI Models SHAP and LIME}
Preexisting explainable AI models such as SHAP (SHapley Additive exPlanations) and LIME (Local Interpretable Model-agnostic Explanations) have emerged as pivotal tools for providing transparency in model decisions. SHAP, rooted in game theory, attributes the contribution of each feature to the prediction of a particular instance, thus offering a detailed decomposition of the model's decision-making process (Lundberg and Lee, 2017). Its strength lies in its consistent and theoretically justified approach, which applies across a variety of model types including deep learning, tree-based models, and ensemble methods.

On the other hand, LIME provides local explanations by approximating the complex model with a simpler, interpretable model around the prediction of interest (Ribeiro et al., 2016). It perturbs the input data to understand how the model's predictions change with slight variations in input features, thus allowing users to comprehend the behavior of black-box models in a localized region of the input space.

Despite their widespread adoption, SHAP and LIME are not without limitations. For instance, the computational cost of SHAP increases with the complexity and size of the model, while LIME's reliance on local surrogate models may lead to inconsistencies in explanation across different instances.

\todo{Problem of evaluation als Überleitung}