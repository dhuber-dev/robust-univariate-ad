\chapter{Literature Review}
\label{ch:literature_review}
Looking back at our initial examples in Section \ref{sec:time_series} of real-world examples of time series. We motivated that time series are ubiquitous and provided examples like stock prices in finance, temperature readings in meteorology and heart rate monitoring in healthcare. With the presented knowledge about anomalies, it is easy to envision how AD could be applied to these fields.

Pai et al. \cite{Pai2005} developed a hybrid model to capture linear and nonlinear patterns of stock price movements. The model was tested on real stock data and can effectively forecast future stock prices based on past observations. While this is certainly interesting for investors it can also be applied to AD to understand market movements for a more stable economic environment.

AD in meteorology becomes even more important in recent years. Models have been developed to detect abnormal climatic conditions caused by global warming. For instance, the DBSCAN algorithm, monitors year-round temperature readings and identifies deviations from the normal seasonal fluctuations \cite{Celik2011}.

A more individual solution was presented by Carrera et al. \cite{Carrera2019}. They leveraged dictionaries to model heartbeats of a specific user as sparse representations. The model than tracks ECG data and can detect anomalous heart beats in an online manner. The efficiency of sparse representations allows running the model on wearable devices, ultimately providing a warning system for patients with high potentials of heart attacks.

% Methodology of LR
In this chapter we introduce a variety of popular algorithms that can be leveraged to achieve results like these. These algorithms are often not trivial and can vary in their structure and approach towards AD, we start by grouping them using different criteria. Each group comes has its set of challenges, which we present from literature. We then follow a structured taxonomy to explain the working principles and differences of the respected algorithms. In the following, we introduce synthetic and real datasets, which were used to benchmark these algorithms. A short section introduces the concept of automated feature extraction before we take a look at recent advances and trends in AD. Lastly, we formulate a summary of our extensive literature review and the research gaps it yielded.

\section{Methodology: How to get an Overview?}
Many different approaches to AD can be found in the literature. To address the diversity of these algorithms it is better to organize them. The diverse set of algorithms for AD can be categorized based on four different criteria: \textit{input dimensionalities}, \textit{learning types}, its \textit{domain} or the underlying \textit{method}.

\begin{itemize}
    % 1. by input dimensionality: univariate vs. multivariate
    \item \textit{Input Dimensionality} is directly related to the categorization explained in Section \ref{sec:time_series}, where we pointed out that time series data can be univariate and multivariate. Some algorithms are designed to work on univariate data exclusively, while other algorithms are optimized to mulitvariate input, potentially of different data types.
    
    % 2. by learning type (supervised vs. semi-supervised (most) vs. unsupervised)
    \item \textit{Learning Types} also divides the models and underlying datasets into supervised, semi-supervised and unsupervised. The later marks the biggest group because it is popular in literature due to the challenge of labeling anomalies.

    % ML, deep learning... Erwähnen, dass danach auch innerhalb methods gruppiert werden kann und irgendwie argumentieren, dass methods interessanter ist.
    \item \textit{Domains} can separate the algorithms based on the field they are coming from. These include statistics, signal analysis, classic machine learning, outlier detection, data mining and deep learning. Each with its own approach to detecting anomalies.
    
    % by method (working principle): model type (ML, deep learning, transformer,...) and categorize them into taxonomy of Schmidl et al.
    \item\textit{Methods} describe the underlying concept used to detect anomalies. The six categories forecasting, reconstruction, distance, encoding, distribution and isolation tree methods are a common separation. They will be used throughout the paper and each category is explained in Section \ref{sec:taxonomy}.
\end{itemize}

\section{Defining the Scope}
From the above groups, we enclose our research on univariate input dimensionality and algorithms of unsupervised and semi-supervised learning types. Taking a look at the literature helps justifying this decision and reveals the limitations it comes with.

\subsection{Focus on Univariate Time Series}
The decision to sorely use univariate approaches to AD can be guided by limited datasets or computational resources. 
\todo{Vorteile univariate}
However, concentrating on univariate AD although multiple variables are available can suffer from performance decrease.
Patcha et al. \cite{Patcha2007} point out that some data instances only appear as anomalies in a combination of attributes. This can be best illustrated using an example: \todo{use real example from literature} A financial institution is monitoring credit card transactions to detect fraudulent activities, by analyzing the amount, location and time of a transaction. A customer often withdraws around 100€. He lives in Berlin, which is usually the location of his credit card transactions, but likes to travel. Normally, he withdraws his money after work. Given this normal behaviour, a withdrawal of 100€ (normal amount) made in Tokyo (where he often travels) at 6 PM Berlin time (usual time), would not raise suspicion. Only when considering these variables together, an anomaly might emerge, since 6 PM in Berlin is 1 AM in Tokyo. This example demonstrates that relying solely on univariate data can overlook critical anomalies.

\subsection{Excluding Supervised Learning}\todo{überarbeiten mit infos aus einem paper}
Unsupervised learning is most popular in AD and, therefore, has a high level of scientific relevance. However, some datasets offer labeled anomalies, which offers great benefits in the detection process.

Several supervised AD algorithms have been proposed in the literature. Many authors incorporate known anomalous samples into the training process in a supervised manner  \cite{Ryzhikov2019, Marteau2017, Li2017}. While they record reasonable accuracy results, it has been found that many supervised approaches struggle with identifying new and unknown anomalies. On the one hand, this can help with the problem of differentiating anomalies from novelties. On the other hand, discovering new anomalies, which were not part of the training data, is often crucial in real-world scenarios \cite{Goernitz2013}. 
Overall, it can be observed in the literature that supervised learning in AD is a good tool to handle a problem's complexity, like highly multivariate input. It is still noted that this comes with the challenge of labeling anomalies.

Semi-supervised methods have emerged as an alternative, where only a small amount of labeled data is needed. These methods are primarily dominated by deep learning approaches. Several papers integrate techniques like \textit{Autoencoders} \cite{Bagel,Donut,IE-CAE} or \textit{Convolutional Neural Network} \cite{SR-CNN} to reconstruct time series. A few papers rely on forecasting time series by utilizing \textit{Echo State Networks} \cite{HealthESN} or \textit{Wavelet Neural Networks} \cite{OceanWNN}. These methods try to bridge the gap by incorporation knowledge about the anomalies in the training process, while not overly relying on it.

Unsupervised methods are very popular because they can be applied to most datasets without the need of labeling. Especially in the field of AD, researchers struggle with labeling data, which is why unsupervised methods dominate for AD \cite{Aggarwal2017}. Anomalies inherently appear infrequently and unpredictably within datasets. As a result, obtaining labeled training data that includes every possible type of anomaly is challenging, if not impossible. This makes fully supervised methods impractical \cite{Pang2021a}. Most researchers rely on methods from the area of classic statistics and data mining. Deep learning methods are arising but are still underrepresented. Braei et al. \cite{Braei2020} have shown that deep learning approaches struggle to achieve the same accuracy as statistical methods for univariate AD applications, while having higher computation times. However, their study also shows that deep learning methods excel in detecting contextual anomalies.

\section{Taxonomy}
\label{sec:taxonomy}
Throughout this work, we use a structured taxonomy based on the six presented method above, which were first presented by Schmidl et al. \cite{Schmidl2022}. This taxonomy is not limited to time series but was developed around this data type. It helps to simplify the complexity of AD, allows a more nuance comparison within each group and supports in the development of more robust detection systems. It further aligns algorithm selection with problem characteristics as we argue that a method's performance depends on the target anomaly kind. Lastly, it helps other researchers to classify their algorithm and project the findings of this research onto their own. Below is a detailed explanation of the taxonomy, which is based on the algorithm's detection approach:

\begin{itemize}
    \item \textit{Forecasting Methods} intend to predict future values based on past observations. These methods typically use a sliding window approach, and the predicted future points solely depend on model knowledge and the values within the preceding window. After observing new values, the model compares them with the predicted ones and marks an anomaly if they deviate significantly. Forecasting methods use different predictive models to capture the normal behavior of the time series.
    % \textbf{Example:} ARIMA models the time series as a combination of autoregression (AR), moving averages (MA), and differencing to make the data stationary. When the actual data deviates from this predicted behavior, it is marked as an anomaly.

    \item \textit{Reconstruction Methods} aim to model the normal behavior of the time series by learning a compressed, low-dimensional representation (latent space) of normal data. These models are typically trained on normal data, and during inference, they attempt to reconstruct subsequences of the time series. Anomalies are detected when the reconstruction error—i.e., the difference between the original and reconstructed data—is large. This approach is based on the assumption that anomalies cannot be effectively reconstructed by a model that only learned normal patterns. Autoencoders, LSTM-VAE (Variational Autoencoder), and MSCRED (Multi-Scale Convolutional Recurrent Encoder-Decoder) are examples of reconstruction methods.
    % \textbf{Example:} In an autoencoder, the time series is compressed into a latent space and then reconstructed. If the error between the input and output is significantly high, this suggests the presence of an anomaly, as the model fails to capture the unusual behavior.

    \item \textit{Distance Methods} focus on measuring the similarity or dissimilarity between subsequences in the time series. Anomalous subsequences are expected to have larger distances from other subsequences when compared with normal behavior. These methods often use clustering or nearest-neighbor approaches to detect anomalies. For instance, k-Means clusters the subsequences of a time series, and subsequences that are far from the cluster centers are considered anomalies. Other methods such as Sub-LOF (Subsequence Local Outlier Factor) and STAMP (Scalable Time-series Anytime Matrix Profile) also belong to this family.
    % \textbf{Example:} In k-Means clustering, normal subsequences will be close to the cluster centers, while anomalous subsequences will have a greater distance to the nearest center, thus indicating outliers.

    \item \textit{Encoding Methods} are similar to reconstruction methods but differ in that they do not attempt to reconstruct the original time series. Instead, they encode subsequences into a low-dimensional latent space and compute anomaly scores directly from the encoded representations. Anomalies are detected based on how subsequences behave within this latent space. The methods in this category, such as GrammarViz and Series2Graph, often use symbolic or probabilistic representations to model normal and anomalous behavior.
    % \textbf{Example:} GrammarViz encodes subsequences into symbolic representations and identifies anomalies based on low coverage or complexity in the inferred grammar rules, considering rare and hard-to-compress subsequences as anomalies.

    \item \textit{Distribution Methods} focus on fitting probabilistic models to the time series data and identify anomalies as points or subsequences that deviate from the expected distribution. These methods estimate the probability distribution of the data and flag observations that fall in the tails of the distribution as anomalies. Techniques such as COPOD (Copula-based Outlier Detection), DSPOT (Dynamic Statistical Peak Over Threshold), and Fast-MCD (Minimum Covariance Determinant) fit different types of distributions to detect outliers or abnormal behaviors in the data.
    % \textbf{Example:} COPOD constructs a copula-based model to estimate the tail probabilities of data points. Points with extremely low tail probabilities are flagged as anomalies, as they deviate significantly from the normal distribution.

    \item \textit{Isolation Tree Methods} are based on the principle of isolating anomalous points more easily than normal points through recursive partitioning. These methods construct a forest of random trees by splitting the data based on randomly selected features and values. Anomalous points, which are easier to isolate, tend to be closer to the root of the tree, resulting in shorter path lengths.
    % \textbf{Example:} Isolation Forest constructs random decision trees, and points that require fewer splits to isolate are considered anomalies. This approach is effective for detecting rare, isolated anomalies in large datasets.
\end{itemize}

% Importance of Categorization
In the following we use this categorization to compare methods, identify their strengths and weaknesses, and suggest the most appropriate technique for a given problem. This classification helps practitioners navigate the growing number of available algorithms, making the selection process more informed and systematic.

\section{Anomaly Detection Algorithms}
In the following we present common methods from the literature. We follow the taxonomy presented earlier to group them by their underlying method to handle data and detect anomalies.

\subsection{Forecasting}
\textit{ARIMA} is a widely known model of the forecasting family, particulary effective on non-stationarity data (ref. Hamilton problem \ref{time_series}). It combines three components: AutoRegression (AR), Integration (I) and Moving Average (MA). 
The Autoregression component intents to predict future values based on past observations. The integration component makes the time series stationary by differencing. The higher the degree of differencing the more the model focuses on changes between data points rather than absolute values. A moving average then accounts for forecasting errors it made in the past to improve the accuracy of future forecasts. By doing so, short-term fluctuations can be captured, which are neglected by the AutoRegression component \cite{ARIMA}. 

\textit{SARIMA} extends ARIMA by adding seasonal components. ARIMA removes non-stationary characteristics by differencing, which involves subtracting the previous value from the current. SARIMA performs seasonal differencing, which subtracts values from a specific number of periods ago (e.g., a 12-month downward trend for yearly seasonality). It also incorporates seasonal support in the AR and MA components, which in reverse makes it more difficult to set up due to its parametric nature. 

The \textit{Triple Exponential Smoothing} (also known as the \textit{Holt-Winters Method}) extends basic exponential smoothing by incorporating seasonality and trend components. It takes a heuristic approach by setting initial values based on the first observations. As new data comes in, the components are updated, either in an additive or multiplicative manner. Once the model is trained, it can forecast future values by combining the components back together \cite{Triple_ES}.

The \textit{Median Method} uses a simpler yet computationally efficient method. The core idea is to use the median as a reference point, as it is more robust to anomalies. This median is calculated based on a moving windows, which consists of the current data point and its neighboring points. The median acts as the forecasted value, meaning the difference between the observed value and the median is compared to a predefined threshold, which marks an anomaly if exceeded \cite{MedianMethod}.

\textit{Random Forests} is a well known concept, which combines multiple decision trees and is used to predict anomalies. It measures how anomalous a data point is based on its path length across different trees. Normal data points tend to have longer paths due to their similarity with many other points, while anomalies have shorter paths as they are more easily separated from normal data \cite{RForest}. 

\textit{XGBoosting} is a gradient boosting technique that builds on random forests. It iteratively adds trees to correct errors made by previous trees, using gradient descent to minimize the loss function. XGBoosting adds regularization, which prevents overfitting by penalizing overly complex tree formations \cite{XGBoosting}.

\textit{Numenta Hierarchical Temporal Memory (HTM)} is a biologically inspired model that learns temporal patterns and is optimized for streaming data. It encodes time series into Sparse Distributed Representations (SPRs)\footnote{A binary vector where only a small part is active (1) at the time, representing specific patterns in the data.}. The encoded SPRs are analyzed by the Temporal Memory for sequential patterns, which are then used for prediction and AD. HTM continuously updates the Temporal Memory, making it more robust to concept drift \cite{NumentaHTM}.

\textit{HealthESN} leverages Echo State Networks (ESN)\footnote{A type of Recurrent Neural Network (RNN) with a fixed internal reservoir that optimizes it for temporal data.} for AD, focusing on the problem of imbalanced datasets. It does this by using the a reservoir with fixed values from random initialization (untrained), which transforms input data into a higher dimensional space, making it rich in temporal information. Only the output layer is then trained using linear regression model \cite{HealthESN}.

\textit{OceanWNN} integrates Wavelet Neural Networks (WNN) with a dynamic threshold, which can handle complex time series, e.g. ocean observation data. It decomposes the time series using wavelets to separate the noise and extract features. The network then uses the wavelet-transformed data to learn patterns, allowing it to capture short- and long-term patterns (extremum vs. trend). During backpropagation, the weights of the wavelet basis function are also updated, which makes the features extraction dynamic \cite{OceanWNN}.


\subsection{Reconstruction}
\textit{Fast Fourier Transformation (FFT)} is a classical approach that transforms the series from time- to frequency-domain. Anomalies are then detected if unusual frequency components appear. Analyzing the frequencies makes FFT effective in identifying periodic anomalies but its performance suffers from non-stationarity \cite{FFT}.

\textit{Spectral Residual (SR)} decomposes the time series into a spectral representation. In the first step it also performs a fourier transformation. The frequency components are then analyzed by their strength (magnitude) by computing the logarithmic amplitude spectrum, which scales down large values, making also subtle anomalies noticeable. In a following step, the spectral residual is calculated to smooth the amplitude spectrum, leaving the \textit{saliency map} where higher values indicate areas of the time series that deviate from the expected frequency pattern. This map highlights normal behavior and anomalies after being transformed back to the time-domain \cite{SR-CNN}. Both FFT and the extension SR can be sensitive to noise as it effects the frequency domain representation. 

\textit{Prediction Confidence Interval (PCI)} uses a forecasting model and calculates a confidence interval around the predicted value. For example, if a model predicts a value of $100$ with a $95\%$ confidence interval of $\pm 10$, the interval would range from $90$ to $110$. Once the actual value is observed it is compared against the predicted confidence interval and if it falls within it, the observed value aligns with the expected behavior \cite{PCI}. Because the confidence interval serves as normal range, the method can be considered a reconstruction method. The confidence interval is calculated dynamically but the method might struggle with highly irregular data, depending on the sliding window.

More advanced methods like \textit{Donut} and \textit{Bagel} tackle this problem by utilizing Variational Autoencoders (VAE) to learn complex, non-linear patterns. Donut uses a sliding window to passes them individual chunks of the time series through the VAE. Based on the reconstruction error (after the decoding stage) it calculates an anomaly score. If the error is above a certain threshold, the data point is classified as an anomaly \cite{Donut}. Bagel builds upon it and uses a Conditional-VAE (CVAE). This allow the model to condition its learning and reconstruction on additional temporal context, which is lost by Donuts sliding window approach. For example, instead of treating each window as an isolated snapshot, Bagel’s CVAE can learn how a data point relates to the time period in which it occurs (e.g., time of day, day of the week) \cite{Bagel}. Both VAE's methods focuses on seasonal data and Bagel's additional temporal awareness makes it more robust to time-based shifts (e.g. trends). However, highly frequent anomalies (e.g. anomalies that span milliseconds in sensor reading) can potentially influence the VAE's latent space. 

On the other hand, deep learning models like \textit{IE-CAE} and \textit{SR-CNN} leverage the image based CNNs for feature extraction. IE-CAE transforms time series into image-like representations with the idea to make complex temporal patterns easier to analyze. The CNN is trained using these images. During inference, new time series are encoded and passed through the CNN. The anomaly score is then calculated by comparing the original encoded image with the reconstructed image  \cite{IE-CAE}. Several techniques, like Gramian Angular Fields (GAF), Recurrence Plots, Spectograms or Markov Transition Fields (MTF) \cite{Wang2015, Goswami2019, Chaurasiya2020}, can be used to encode time series into a visual format.
\textit{SR-CNN} uses the previously presented SR method to encode time series. The saliency map, containing normal patterns and anomalies, is used to train the CNN. During training, sections of the saliency map are classified as normal or anomalous and when applying new data the trained CNN refines the AD from SR by analyzing the patterns in the saliency map. This two-step process allows SR-CNN to benefit from both the signal analysis capabilities of SR and the pattern recognition capabilities of CNNs \cite{SR-CNN}.


\subsection{Distance}
\textit{NoveltySVR} uses Support Vector Regression (SVR) \cite{Cristianini2008} for AD. It models the typical behavior of a time series using SVR and then identifies deviations by measuring the distance between observed data points and the model's predicted values. If the deviation exceeds a predefined threshold, the subsequence is flagged as anomalous. This method serves as a foundation for understanding how regression-based techniques can be adapted for AD \cite{NoveltySVR}.

\textit{PS-SVM} builds on the SVR concept by incorporating SVMs and phase space representation, which is a multi-dimensional space. In this space, each point represents a unique state of the system and changes are traced as a trajectory in this space \cite{Conti2023}. The time series is transformed into such a phase space and an SVM detects anomalies by assessing distances between data points in this space \cite{PS-SVM}.

\textit{Heaviest Occurring Time Series using Symbolic Aggregate approXimation (HOT SAX)} uses a symbolic approach to distance methods, like many encoding methods, where subsequences of the time series are transformed into symbolic representations using \textit{Symbolic Aggregate approXimation (SAX)} \cite{SAX}. HOT SAX calculates distances between SAX words and identifies \textit{discords}, which are words with the largest minimum distance to another word. These words are potential anomalies as they are least similar to all others. HOT SAX aims to find the heaviest discord among them, which is the one with the greatest distance. Hereby, the distance of words can be understood as a measure how different words are \cite{HOT_SAX}.

\textit{NormA-SJ} first models a normal behavior. A sliding window approach delivers subsequences, which are then compared by the \textit{similarity join (SJ)} algorithm. SJ identifies pairs of subsequences that are similar to each other based on a distance metric (e.g. Euclidean distance). By comparing each subsequence, the method identifies patterns that occur frequently, thus establishing a model for normal behavior. In a second step, NormA-SJ then calculates the distance of each subsequence to the learned normal behavior. Patterns that are far away are considered anomalous. Using a refined distance allows the model to rank the detected anomalies, making it possible to identify multiple anomalies or prioritize significant ones \cite{NormA-SJ}.

\textit{STAMP} and \textit{STOMP} leverage the SJ algorithm as well to make use of the calculated distances by creating a matrix profile. This profile contains the similarities between all possible pairs of subsequences across the time series. STOMP improves upon STAMP by optimizing the calculation, allowing faster identification of motifs (repeated patterns) and discords (anomalies) based on their similarity distances. Because they do not model a normal behavior, like NormA-SJ, both methods are parameter-free \cite{STAMP, STOMP}.

\textit{VALMOD} extends the matrix profile framework by introducing variable-length motif and discord discovery. Unlike fixed-length approaches, like STAMP and STOMP, VALMOD searches for similar subsequences of varying lengths and identifies those with the smallest or largest distances across the time series. It starts by analyzing shorter lengths and gradually explores longer lengths, using insights gained from shorter-length analysis. This approach together with a pruning technique helps handling the bigger search space and the computational complexity. The flexibility to define a range of subsequence lengths rather than a fixed one, allows VALMOD to detect anomalies at different scales (e.g. extremum vs. trend) \cite{VALMOD}.

\textit{SAND} is built upon STAMP and STOMP but designed for streaming data. It introduces a mechanism to incrementally update the matrix profile as new data arrives. SAND dynamically recalculates the SJ, while only focusing on the areas that are effected by the new data \cite{SAND}.

\textit{Left-STAMPi} is an incremental variant of STAMP, also focusing on streaming data scenarios. It combines the efficiency of STOMP's distance calculations with the streaming capabilities of methods like SAND, ensuring that the matrix profile stays accurate as new data arrives. Left-STAMPi shows many similarities with SAND but can be used for both, AD and motif discovery in streaming contexts \cite{STAMP}.

\textit{Singular Spectrum Analysis (SSA)} aims to decomposes the time series first and then intends to identify patterns. The decomposition allows the model to treat the components (ref. $T$, $S$ and $N$ in Equation \ref{eq:decomposition}) individually. The first step in SSA is to create a \textit{trajectory matrix}\footnote{It is a Hankel matrix, where each row (or column) represents a shifted (lagged) version of the time series, making it possible to analyze the relationships between different parts of the series.} using embedding, which captures the relationships between different parts of the time series. In a following step, the trajectory matrix is decomposed into a sum of simpler matrices using Singular Value Decomposition (SVD). In the decomposed stage, the components are grouped and separated into dominant components (like trends and seasonalities) and less significant components (like noise). Lastly, the selected components are reconstructed back, resulting in individual time series that represent trend, seasonal patterns and residual noise. Anomalies are then detected in the residual component using distance measures \cite{SSA}.

\textit{Local Outlier Factor (LOF)} has also been adapted to AD. First, it finds a fixed number of nearest neighbors for each data point. Then it normalizes the distances to each nearest neighbor using reachability distance. Lastly, it evaluates the density around each point and compares it to their nearest neighbors. This essentially identifies regions of significantly lower density compared to their surroundings, making it effective for detecting anomalies \cite{Sub-LOF}.


\subsection{Encoding}
\textit{GrammarViz} and \textit{EnsembleGI} are both based on grammar induction, where time series are first converted into a sequence of discrete symbols. For example, temperature readings could be converted into "aabbcc" based on a predefined threshold. The grammar induction algorithm then groups repeating patterns (substrings) into rules, basically creating a grammar. This represents a set of rules, which are then mapped back to the time series. If a sequence does not follow the rule set of the grammar, it is flagged as an anomaly \cite{GrammarViz}.
EnsembleGI tries to overcome the sensitivity of parameters, which has been a challenge for GrammarViz, by using multiple sets of parameters and aggregate the results in the rule learning process \cite{EnsembleGI}.

\textit{Series2Graph} encodes time series data into a graph representation, rather than symbols. Subsequences of the time series are converted into low-dimensional vectors using a sliding window. Similar vectors are then put into nodes, representing recurring patterns. The edges correspond to how the patterns evolve over time. E.g. pattern A is usually followed by pattern B in the time series, then an edge is drawn between the two nodes representing A and B. If a new subsequence is processed and it does not fit into the existing graph, it is considered an anomaly \cite{Series2Graph}.

\textit{Time-series Anomaly Recognition using Z-normalization And Novelty detection (TARZAN)} combines grammar induction and graph theory. It creates a sequence of symbols using Z-normalization and creates a suffix tree from them. Each node represents a pattern and the algorithm stores how often the pattern has appeared in the past. Edges represent transitions between the patterns, like in the Series2Graph method. Using a Markov model, TARZAN estimates the likelihood of the edges, i.e. how often a symbol (node) transitions into another. When a new subsequence is encountered, the model uses the transition probabilities to estimate the expected frequency of this new pattern. This estimate is then compared with the observed frequency of the subsequence. If there is a significant difference, it is considered an anomaly. In other words: using an expected frequency allows TARZAN to determine how surprising or novel a subsequence is \cite{Tarzan}.

\textit{TSBitmap} takes a similar approach in that it uses symbolic representations. The symbols are also put into a visual form, for which TSBitmap uses bitmaps. These bitmaps are a 2D grid where each cell represents the frequency of a pattern, essentially visualizing a distribution. This offers a user the possibility to visually inspect new sequences and identify cells or regions with unusual frequency. The algorithm also provides automated scoring where cells with very low or high frequencies are flagged as anomalous patterns \cite{TSBitmap}.

\subsection{Distribution}
\textit{Fast-MCD} is based on the \textit{Minimum Covariance Determinant (MCD)} method, which aims to find the tightest cluster of points in the dataset. This subset of the dataset is identified by the smallest possible determinant of the covariance matrix and is assumed to be free of outliers \cite{Hubert2017}. Fast-MCD then uses faster algorithms to calculate an estimate of the mean (cluster center) and the covariance matrix (scatter). It then weights all the data points, giving more weight to those closer to the mean \cite{Sub-Fast-MCD}.

\textit{S-H-ESD} is also based on a classical approach the \textit{Extreme Studentized Deviate (ESN)} \cite{Nair1948} and combines it with seasonal decomposition (S), making it a hybrid (H) approach. The decomposition stage separates the trend $T$, seasonal $S$ and residual $R$ components of the time series using LOESS, as shown in Equation \ref{tsa_gleichung}. It then removes $S$ and $T$ components and assumes $R$ is more likely to contain anomalies. In a second stage, the S-H-ESD applies the ESD-test on the residuals to detect anomalies \cite{S-H-ESD}. The ESD test is used for identifying outliers in a univariate dataset by evaluating how far data points deviate from the mean (or in this case, the median) \cite{Nair1948}.

\textit{DWT-MLEAD} also uses a decomposition stage. The \textit{Discrete Wavelet Transform (DWT)} decomposes time series into high-frequency (e.g. extremum anomalies) and low-frequency (e.g. long-term trends) components of the time series. DWT is performed on sliding windows resulting in a feature vector for each window. DWT-MLEAD then assumes that these vectors follow a Gaussian distributions, which can be estimated using Maximum Likelihood Estimation. Anomalies are then identified based on deviations from these estimated distributions \cite{DWT-MLEAD}.

\textit{Dynamic Streaming Peaks-Over-Threshold (DSPOT)} is based on the \textit{Extreme Value Theory (EVT)} \cite{Haan2006}, which allows it to automatically compute thresholds for detecting anomalies without making assumptions about the underlying data distribution. After processing an initial segment of the time series a high threshold $t$ is estimated. It then follows its POT approach and fits a \textit{Generalized Pareto Distribution (GPD)} using MLE to calculate a new threshold $z_q$. If a data point exceeds the threshold $z_q$ it is considered an anomaly. If it exceeds $t$ but not $z_q$ it is used to update the GPD parameters, making DSPOT adaptable to concept drifts.

\subsection{Trees}
\textit{PST} uses a combination of the Markov model and a suffix tree, like TARZAN. PST, however, is a probabilistic model that captures variable-length dependencies between symbols in a sequence. This means it adjusts the length of the considered history based on the data, like a variable-order Markov model. It builds a tree structure where each node represents a suffix of previously observed symbols and contains the probabilities of the next symbols. The related edges indicate the possible next symbols. As the trees contain the possible combination of symbols, an additional pruning step is applied for efficiency. Anomalies are then detected by identifying subsequences whose probabilities are significantly lower than expected \cite{PST}.

\textit{iForest} assumes that anomalies are easier to isolate because normal data points are more densely packed. It uses isolation trees to recursively partition the data, based on random split points. The process continues until each data point is isolated (or a stopping criterion is met). The more splits were needed per data point the bigger the isolation tree is, i.e. given the assumption that anomalies need less splits, they should appear closer to the tree's root. This is evaluated using the average path length (root to data point) of all trees in the forest, which leaves an anomaly score \cite{Sub-IF}.

\subsection{The best-performing Algorithm}
The diversity of methods also highlights the fact that no single algorithm is universally superior. It rather suggests that the choice of algorithm should depend on the specific characteristics of the dataset and the type of anomalies being targeted. 

We covered a prominent set of methods with good generalization and performance results from the literature. However, the chosen methods are a very small subset of available methods developed by researchers from various domains. A thorough literature review has shown that many algorithms are tailored to a specific problem and dataset, without the intention to apply it differently. 

The above presentation of methods explains this further by showing that the "best" method depends on various factors such as the nature of the data, type of anomalies, computational constraints, etc. \cite{Schmidl2022}. According to the reviewed literature, no single of the reviewed methods universally outperforms others across all scenarios, as each algorithm has its strengths and weaknesses.

To name examples, classical statistical methods like ARIMA and SARIMA are efficient for detecting anomalies in time series with clear seasonality and trends but struggle with more complex non-linear patterns. Machine learning approaches like Random Forests or XGBoosting tend to perform well with non-linear relationships and can adapt to diverse types of data, but are very parameter sensitive. On the other hand, deep learning models such as WNNs promise to capture more complex temporal dependencies but may require more data. 

Therefore, the choice of the "best" method in literature has always been guided by the requirements of the use case. Additionally, several researchers have evaluated the promised strengths of these methods on different benchmarks \cite{Schmidl2022, Braei2020, BlazquezGarcia2021, Chandola2009}. These comparisons showed that some promises can not, or are only met after careful parameter tuning. However, the nature of some algorithms allows them to outperform others in certain aspects and knowing them, enables future researchers to find the best method for their problem.


\section{Datasets}
In real-world data, different anomaly kinds can appear or even overlap. Synthetic datasets allow the isolation of anomalies, which can help to evaluate algorithm’s capability on distinct anomaly kinds. Wenig et al. \cite{Wenig2022} introduced a dataset generator, which allows researchers to control specific characteristics and anomaly kinds, providing well-labeled data. Its generation process includes the creation of base oscillations (e.g. sine, polynomial, or chaotic waveforms), which can be combined with user-defined trends and random noise to build time series with desired complexity. After the base time series is created, anomalies are injected based on the user's specifications. GutenTAG supports 10 kinds of anomalies, which include variations in mean, pattern shifts, trends, and frequency. These anomalies manipulate the time series, allowing for controlled and diverse testing conditions.

The evaluations results from Schmidl et al. \cite{Schmidl2022} offer a comprehensive study of various time series AD algorithms. This dataset covers performance results on 976 real-world and synthetic time series datasets, with metrics captured on 71 distinct algorithms from different families. The dataset groups algorithms into the families presented above. It includes real datasets from various domains and synthetic datasets generated using the GutenTAG library, which accounts for 22\% of the performance results.
The dataset uses multiple threshold-independent performance metrics like AUC-ROC, Precision-Recall (AUC-PR) and Precision-Recall under Range-based Threshold (AUC-PTRT). These metrics help to compare algorithm performance without needing to define a specific threshold.
Overall the dataset can provide insights into each algorithm's strengths and weaknesses on various anomaly types, facilitating an informed selection of optimal algorithms for time series AD.

\input{plots/distribution_plots_pie_bar/overall_dist_plot}

Figure \ref{fig:pie_overall_tax} reveals the separation of the dataset by algorithm families. It can be seen that the families are not equally represented in the dataset. There is a clear dominance of distance-based algorithms within the dataset, followed by forecasting and reconstruction methods, which together make up the same representation of distance-based methods. Distribution, encoding and tree methods are less explored in the study.

Figure \ref{fig:pie_overall_anomaly} separates the same dataset but by anomaly kinds. We note that variance related anomalies are heavily featured. Frequency and extremum anomalies are also significantly represented but when added together still contain less samples than variance alone. The other anomaly kind have less than 1000 samples each. \todo{explain that multiple anomalies in one time series and this number counts each anomaly}


\section{Feature Extraction}
Christ et al. addressed the problem of filtering relevant features from a large set of potential features with the FRESH (FeatuRe Extraction based on Scalable Hypothesis tests) algorithm. They integrated common feature extraction methods with a feature importance filter. This filter applies non-parametric hypothesis tests to evaluate the importance of each feature. When working with different datasets, where domain knowledge can be limited, FRESH can assist in the feature selection process \cite{fresh}. \todo{Add LOESS}
Christ et al. later implemented the FRESH algorithm in an open-source Python package. It automates feature engineering by, first, extracting a large number of features from time series data and, second, performing feature selection to identify statistically significant features. In total, it combines 63 time series characterization methods, which compute a total of 794 features. The selection process is then covered by the FRESH algorithm, which performs hypothesis tests, based on the significance of a feature for a given problem type (classification or regression). 
FRESH and tsfresh have been evaluated on benchmark datasets including a selection of binary classification problems from the UCR time series classification archive \cite{UCRArchive2018} as well as time series from industrial applications. The evaluation demonstrates that, given a large feature set and large time series samples, the algorithm often outperforms Boruta \cite{boruta} based feature selection approaches \cite{boruta_ex1} and Dynamic Time Warping \cite{dtw} based approaches \cite{dtw_ex1, dtw_ex2}. The parallel nature of the algorithm, further, makes it scalable to complex machine learning pipelines. Because of its easy integration paired with its benchmark results and scalability tsfresh is a popular tool for feature engineering in time series analysis \cite{tsfresh_ex1,tsfresh_ex2,tsfresh_ex3,tsfresh_ex4,tsfresh_ex5}. 
The previously presented dataset, created with the GutenTAG library has the benefit of shorter time series (10.000 datapoints), which is an important characteristic since  tsfresh struggles with huge datasets.

\section{Recent Advances and Trends}
% Deep Learning, LSTM, CNNs, Autoencoders
Recent advances and trends in AD, particularly in time series data, show a growing interest in machine learning and deep learning techniques, apart from traditional statistical methods. Deep learning has become increasingly prominent due to its ability to capture complex temporal dependencies. Techniques such as LSTMs, CNNs \cite{SR-CNN} and Autoencoders \cite{IE-CAE} show improvements in detecting non-linear patterns and long-range dependencies in time series \cite{Braei2020, Schmidl2022}.

% Transformers
However, these techniques are often limited in their capabilities to capture long-term dependencies due to their sequential nature and high computational costs. This led to the idea of using the self-attention mechanism of transformers to process entire sequences at once, which can model both short- and long-range dependencies. Research on models like TranAD (Transformer-based AD) showcases this potential, as it applies attention-based sequence encoders and adversarial training for AD, achieving high accuracy and fast training times \cite{Tuli2022}. A challenge for transformers, over e.g. traditional statistical methods, is still the computational overhead, which forms another research interest in the field. Frameworks like UTRAD (U-TRansformer based AD) use skip connections to accelerate the detection process \cite{Chen2022}.

% Energymodelle
Some energy-based models have also emerged in recent years as an approach towards handling the complex data structures. These models approach AD by learning the underlying energy landscape of normal data, from which anomalies then identify as points with high energy scores or reconstruction errors. Deep Structured Energy-Based Models leverage various neural networks, like CNNs for spatial and RNNs for sequential data, to fit diverse data structures \cite{Zhai2016}. 
A framework for video AD focuses on unsupervised learning by using generative approaches to capture the normal distribution. Deviations are then identified using Boltzmann machines, ultimately reducing feature sensitivity \cite{Vu2017}.

% Prefer Unsupervised
When analyzing recent literature, it also becomes apparent that many methods focus on unsupervised learning. It can be argued that the adaption of models and frameworks to AD, is not only motivated by their performance results from other fields, but also by the difficulty to obtain labeled data. Many upcoming methods aim to learn normal patterns from unlabeled datasets rather than relying on labels, like reconstruction based autoencoders \cite{IE-CAE, Cao2023}.

% Distribution shift problem
A key advancement is the growing research on the \textit{distribution shift problem} \cite{distshift1,distshift2,distshift3,distshift4,distshift5}. This occurs when the training and test data come from different distributions. Many existing methods assume that the test and training data come from the same distribution, which can lead to bad performance if this assumption is not met. New approaches are being developed to address the distribution shift and, ultimately, improve out-of-distribution generalization. Methods like the Generalized Normality Learning (GNL) framework are emerging to to tackle this problem \cite{Cao2023}.

% Benchmarking
Lastly, there has been a trend of benchmarking methods, analyzing the strengths from various algorithm families. Comprehensive evaluations involving many algorithms, such as statistical methods, ml models and deep learning architectures have been conducted on large, diverse datasets \cite{Schmidl2022}. This helps identifying the strengths and weaknesses of each approach, offering a clearer picture for future research.


\section{Summary and Gaps in Existing Research}
% Summary
The literature review shows that researchers are dealing with AD and the field of time series analysis in general since a long time. It also shows that the research domain is still very active, with new methods arising consistently. We introduced a taxonomy to structure this vast variety of approaches towards AD. We showed that within a family of methods, there are different approaches from all kinds of eras, like the forecasting family with statistical and deep learning approaches. We introduced benchmarking datasets and a library to automate feature extraction on time series.

% Time Series Data
Blazquez-Garcia et al. point out that relatively few studies focus on AD in time series data. Many methods perform well on static data but fail to account for temporal dependencies \cite{BlazquezGarcia2021}. Many methods that build on other data types can be adapted to temporal data. However, while these methods might perform well on identifying point anomalies, they will struggle with detecting collective or contextual anomalies \cite{Braei2020}. In recent years, more research has been done on AD in time series but there is still a high potential in optimizing methods towards temporal dependencies.

% Distribution Shifts
Another, directly related research gap in AD is the inability of many methods to handle distribution shifts between training and test datasets. These can be caused by phenomena like changing environmental conditions (seasonality) or evolving system behavior (trend) and are very common in real world time series data. Some methods comprehend these non-stationary time series while others do not \cite{Cao2023}. The probability of such events asks for a higher focus on distribution shifts in the field. Another challenge regarding a distribution shift is the distinction between anomalies and novelties. Novelties can be confused with anomalies ones they are new but may become part of the normal distribution over time. Many algorithms struggle with the distinction, which highlights another research gap \cite{Chandola2009}.

% Robustness + 
Given that numerous methods already tackle temporal dependencies, distribution shifts, and other problem characteristics, there is minimal need for new models, highlighting a limited research gap. But, it highlights that despite the amount of research, critical gaps remain in the robustness of algorithms across all domains. Many algorithms are equipped with special structures and are tailored towards a specific problem characteristics. The main research gap resulting from this is lack of comprehensive evaluations, particularly, between techniques that are tailored to different problem characteristics. As a direkt result, the method's performances can not be assessed successfully across diverse datasets \cite{Braei2020}.